{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d3dwhW5RieN"
      },
      "source": [
        "# Deep Learning Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu3XCXRZRoSN"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mmyNZJq2PzDk"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    if os.getcwd() == '/content':\n",
        "        !git clone 'https://github.com/inb-luebeck/cs5450.git'\n",
        "        os.chdir('cs5450')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0dIxEpPCdyG"
      },
      "outputs": [],
      "source": [
        "#making sure livelossplot is installed\n",
        "try:\n",
        "    import livelossplot\n",
        "except ModuleNotFoundError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install livelossplot==0.4.1\n",
        "\n",
        "#making sure huggingface datasets is installed\n",
        "try:\n",
        "    import datasets\n",
        "except ModuleNotFoundError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHZacje2SIkr"
      },
      "source": [
        "## Setup\n",
        "This exercise can utilize GPU acceleration. If you are using Google Colab you can enable access to a cloud GPU by selecting from the menu above:\n",
        "\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU**\n",
        "\n",
        "If you are running this notebook on your own machine, GPU acceleration is available if you have an Nvidia GPU and a CUDA-enabled driver installed. Otherwise calculations will run on the CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUEN_wNrSSEb"
      },
      "source": [
        "## Real Cats vs AI-Generated Cats\n",
        "In this exercise, we will use deep learning to solve a real-world problem: classifying whether a given cat photo is real or ai generated. To make things more interesting, this exercise sheet will be carried out as a competition.\n",
        "\n",
        "You are given a dataset of 900 labeled cat photos with labels \"fake\" (0) or \"real\" (1), as well as 86 test images without labels*.\n",
        "\n",
        "The next cells provide code to get you started. It contains a fully functional implementation of a simple convolutional neural network (CNN). It has a baseline accuracy of ~74% on the validation set.\n",
        "\n",
        "Your task is to modify or extend the code in order to optimize the classification capabilities of the network.\n",
        "\n",
        "The output of the network is a class-score:\n",
        "- The more negative the output, the more confident the model is that the sample should be classified as class 0 (fake).\n",
        "- The more positive the output, the more confident the model is that the sample should be classified as class 1 (real).\n",
        "- A score of 0 means the model is undecided (which will be treated as predicted class 1 in the accuracy calculation).\n",
        "\n",
        "The class-score is mapped to a class-probability using the sigmoid function. However, for better numerical stability the sigmoid function is already built-in into the `BCEWithLogitsLoss` loss function that is used here, so you don't need to apply sigmoid to the network output during loss calculation.\n",
        "\n",
        "<sub>\\*The **test set** labels stored in the dataset are dummy labels, not the actual labels.</sub>\n",
        "\n",
        "### Competition rules:\n",
        "The network has to be trained from scratch (i.e. no pretrained neural network that was already trained on other data) and training is only allowed with the provided data, no external datasets.\n",
        "\n",
        "The winning team is determined based on the test set accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDZChUO5VWja"
      },
      "source": [
        "## Hints\n",
        "In case you are struggeling with the task, here are some helpful tips and hints how the classifier could be improved:\n",
        "1. Useful pytorch [overview](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py).\n",
        "2. Adjust the hyperparameters for the training.\n",
        "3. [Augment](https://pytorch.org/docs/stable/torchvision/transforms.html) the training data (e.g., mirroring).\n",
        "4. Make the network deeper (e.g. more convolution layers or add other layers or change the channel count).\n",
        "5. Regularize your network (e.g. with dropout layers or weight decay).\n",
        "6. Use cross-validation instead of a fixed train-validation split.\n",
        "7. Train multiple networks and combine them in an ensemble.\n",
        "8. Adjust the batch size.\n",
        "9. Change the learning rate and/or weight decay over the course of the training (see `update_learning_rate` below).\n",
        "\n",
        "## Further notes and remarks:\n",
        "1. Each team has to choose a team name. This will be the anonymous identifier when\n",
        "the competition results are published."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ikc2r2mPoK1"
      },
      "outputs": [],
      "source": [
        "# Data loading and preparation\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# load the dataset from huggingface (https://huggingface.co/datasets/MMM-J/real-vs-fake-cats)\n",
        "ds = load_dataset('MMM-J/real-vs-fake-cats', keep_in_memory=True)\n",
        "ds_orig = ds # keeping a copy of the original un-transformed dataset to display some samples\n",
        "\n",
        "def transformations(sample):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),  # Converts images to (C, H, W) format and scales pixel values to [0, 1]\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Example normalization step\n",
        "    ])\n",
        "    sample[\"pixel_values\"] = [transform(image.convert(\"RGB\").resize((112, 112))) for image in sample[\"image\"]]\n",
        "    return sample\n",
        "\n",
        "ds = ds.map(transformations, keep_in_memory=True, batched=True, remove_columns=[\"image\"])\n",
        "\n",
        "train_val_split = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
        "ds = DatasetDict({\n",
        "    'train': train_val_split['train'],\n",
        "    'val': train_val_split['test'],\n",
        "    'test': ds['test'],\n",
        "})\n",
        "\n",
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "print(f\"Using device type: {device_type}\")\n",
        "ds = ds.with_format(\"torch\", device=device)\n",
        "\n",
        "\n",
        "class InMemoryDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        super(InMemoryDataset, self).__init__()\n",
        "\n",
        "        self.data = dataset['pixel_values']\n",
        "        self.labels = dataset['label']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {'pixel_values': self.data[index], 'label': self.labels[index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Since the dataset fits into memory, we keep it there to remove the data loading bottleneck\n",
        "train_data = InMemoryDataset(ds['train'])\n",
        "val_data = InMemoryDataset(ds['val'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJcdE_RCnrmt"
      },
      "outputs": [],
      "source": [
        "def label_to_string(label):\n",
        "  return ds_orig[\"train\"].info.features[\"label\"].int2str(label)\n",
        "\n",
        "# Show real a sample\n",
        "sample_index = 453\n",
        "# Get the training sample for that index\n",
        "sample = ds_orig[\"train\"][sample_index]\n",
        "# Print the label string\n",
        "print(\"Label: \" + label_to_string(sample[\"label\"]))\n",
        "# Display the image\n",
        "display(sample[\"image\"])\n",
        "\n",
        "\n",
        "# Show fake a sample\n",
        "sample_index = 182\n",
        "# Get the training sample for that index\n",
        "sample = ds_orig[\"train\"][sample_index]\n",
        "# Print the label string\n",
        "print(\"Label: \" + label_to_string(sample[\"label\"]))\n",
        "# Display the image\n",
        "display(sample[\"image\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAMUGUgrVGZ8"
      },
      "outputs": [],
      "source": [
        "# The Neural Network architecture\n",
        "\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu0 = nn.LeakyReLU()\n",
        "\n",
        "        self.adaptPool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.fc = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.adaptPool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq6Zv1d9VyQT"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.5\n",
        "weight_decay = 0\n",
        "epochs = 900\n",
        "batch_size = 256\n",
        "\n",
        "# Model and optimizer setup\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "parameters = model.parameters()\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "        parameters,\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "# Utility function to change the learning rate during training\n",
        "def update_learning_rate(new_lr):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = new_lr\n",
        "\n",
        "class StatsTracker:\n",
        "    def __init__(self):\n",
        "        self.loss = 0\n",
        "        self.acc = 0\n",
        "        self.total_sample_count = 0\n",
        "\n",
        "    def update(self, new_loss, new_corrects, sample_count):\n",
        "        new_total_sample_count = self.total_sample_count + sample_count\n",
        "        self.loss = (self.loss * self.total_sample_count + new_loss * sample_count) / new_total_sample_count\n",
        "        self.acc = (self.acc * self.total_sample_count + new_corrects) / new_total_sample_count\n",
        "\n",
        "        self.total_sample_count = new_total_sample_count\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(ds['test'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "liveloss = PlotLosses(plot_extrema=False)\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # training\n",
        "    model.train()\n",
        "    train_stats = StatsTracker()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        out = model(batch['pixel_values'])\n",
        "        loss = loss_func(out, batch['label'].float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # track loss and accuracy\n",
        "        preds = (torch.sigmoid(out) >= .5).float()\n",
        "        corrects = torch.sum(preds == batch['label']).cpu().item()\n",
        "        train_stats.update(loss.cpu().item(), corrects, len(batch['label']))\n",
        "\n",
        "    logs = {\n",
        "        'train_loss': train_stats.loss,\n",
        "        'train_acc': train_stats.acc\n",
        "    }\n",
        "\n",
        "    # evaluation on the validation set\n",
        "    model.eval()\n",
        "    val_stats = StatsTracker()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_loader):\n",
        "            out = model(batch['pixel_values'])\n",
        "            loss = loss_func(out, batch['label'].float())\n",
        "\n",
        "            # track loss and accuracy\n",
        "            preds = (torch.sigmoid(out) >= .5).float()\n",
        "            corrects = torch.sum(preds == batch['label']).cpu().item()\n",
        "            val_stats.update(loss.cpu().item(), corrects, len(batch['label']))\n",
        "\n",
        "    # log the loss and accuracy on the training and validation data\n",
        "    logs = {\n",
        "        **logs,\n",
        "        'val_loss': val_stats.loss,\n",
        "        'val_acc': val_stats.acc\n",
        "    }\n",
        "\n",
        "    liveloss.update(logs)\n",
        "\n",
        "    # draw the graphs only on every 50th epoch to reduce the performance penalty by the visualization during training\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        liveloss.draw()\n",
        "\n",
        "print(f\"\\nFinal training accuracy: {train_stats.acc}\\nFinal validation accuracy: {val_stats.acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyINu-XYCdyH"
      },
      "outputs": [],
      "source": [
        "# Make predictions for the test set\n",
        "\n",
        "# TODO: pick a team name\n",
        "team_name=''\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "\n",
        "all_test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        out = model(batch['pixel_values'])\n",
        "\n",
        "        # collect the predictoins\n",
        "        preds = (torch.sigmoid(out) >= .5).float()\n",
        "\n",
        "        all_test_predictions.append(preds)\n",
        "\n",
        "all_test_predictions = torch.concat(all_test_predictions)\n",
        "\n",
        "# save as csv\n",
        "df = pd.DataFrame(data={\n",
        "    'predictions':all_test_predictions.cpu().tolist()\n",
        "})\n",
        "df.to_csv(team_name + '_submission.csv')\n",
        "\n",
        "# show download link or trigger download\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import files\n",
        "    files.download(team_name + '_submission.csv')\n",
        "else:\n",
        "    from IPython.display import FileLink\n",
        "    FileLink(team_name + '_submission.csv', result_html_prefix=\"Click here to download: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh8gkPAQZZZN"
      },
      "source": [
        "# Your submission file should be named [team_name]_submission.csv!\n",
        "Please upload the csv file and the ipynb file of this notebook (\"File\"->\"Download\"->\"Download .ipynb\") for the challenge in the Moodle course.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}